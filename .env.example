# ChunkHound Environment Configuration
# 
# This file contains all environment variables that ChunkHound supports.
# Copy this file to .env in your project directory and uncomment/set the values you need.
# 
# Configuration Precedence (highest to lowest):
# 1. CLI arguments
# 2. Environment variables (set in your shell)
# 3. .env file (this file)
# 4. Config files (.chunkhound.json, --config)
# 5. Default values

# ==============================================================================
# EMBEDDING CONFIGURATION
# ==============================================================================

# Provider: openai, voyage, mistral, or custom endpoint
# CHUNKHOUND_EMBEDDING__PROVIDER=openai

# API Key for embedding provider
# CHUNKHOUND_EMBEDDING__API_KEY=sk-your-api-key-here

# Model name to use for embeddings
# CHUNKHOUND_EMBEDDING__MODEL=text-embedding-3-small

# Base URL for custom embedding endpoints
# CHUNKHOUND_EMBEDDING__BASE_URL=http://localhost:8001

# Reranking Configuration (optional)
# CHUNKHOUND_EMBEDDING__RERANK_MODEL=rerank-v2
# CHUNKHOUND_EMBEDDING__RERANK_URL=http://localhost:8002/rerank
# CHUNKHOUND_EMBEDDING__RERANK_FORMAT=auto  # auto, cohere, or tei
# CHUNKHOUND_EMBEDDING__RERANK_BATCH_SIZE=100

# ==============================================================================
# LLM CONFIGURATION (for research/map/autodoc features)
# ==============================================================================

# Primary LLM Provider: openai, anthropic, openai-compatible, gemini, mistral, or codex-cli
# CHUNKHOUND_LLM_PROVIDER=openai

# LLM API Key
# CHUNKHOUND_LLM_API_KEY=sk-your-llm-api-key

# Base URL for custom LLM endpoints
# CHUNKHOUND_LLM_BASE_URL=http://localhost:11434/v1

# Per-Role Provider Configuration (optional - override default provider for specific roles)
# CHUNKHOUND_LLM_UTILITY_PROVIDER=openai
# CHUNKHOUND_LLM_SYNTHESIS_PROVIDER=anthropic

# Per-Role Model Configuration
# CHUNKHOUND_LLM_UTILITY_MODEL=gpt-4o-mini
# CHUNKHOUND_LLM_SYNTHESIS_MODEL=claude-3-5-sonnet-20241022

# Reasoning Effort (for providers that support it: low, medium, high)
# CHUNKHOUND_LLM_CODEX_REASONING_EFFORT=medium
# CHUNKHOUND_LLM_CODEX_REASONING_EFFORT_UTILITY=low
# CHUNKHOUND_LLM_CODEX_REASONING_EFFORT_SYNTHESIS=high

# Map/HyDE Configuration (for semantic code mapping)
# CHUNKHOUND_LLM_MAP_HYDE_PROVIDER=openai
# CHUNKHOUND_LLM_MAP_HYDE_MODEL=gpt-4o-mini
# CHUNKHOUND_LLM_MAP_HYDE_REASONING_EFFORT=low

# Autodoc Cleanup Configuration
# CHUNKHOUND_LLM_AUTODOC_CLEANUP_PROVIDER=openai
# CHUNKHOUND_LLM_AUTODOC_CLEANUP_MODEL=gpt-4o-mini
# CHUNKHOUND_LLM_AUTODOC_CLEANUP_REASONING_EFFORT=low

# ==============================================================================
# DATABASE CONFIGURATION
# ==============================================================================

# Database provider: duckdb or lancedb
# CHUNKHOUND_DATABASE__PROVIDER=duckdb

# Database path (defaults to <project_root>/.chunkhound/db)
# CHUNKHOUND_DATABASE__PATH=/path/to/database
# Maximum database size in GB before indexing stops
# CHUNKHOUND_DATABASE__MAX_DISK_USAGE_GB=50

# LanceDB-specific settings
# CHUNKHOUND_DATABASE__LANCEDB_INDEX_TYPE=auto
# CHUNKHOUND_DATABASE__LANCEDB_OPTIMIZE_FRAGMENT_THRESHOLD=100

# ==============================================================================
# INDEXING CONFIGURATION
# ==============================================================================

# Force reindex of all files (ignore modification timestamps)
# CHUNKHOUND_INDEXING__FORCE_REINDEX=false

# File patterns to include (comma-separated)
# CHUNKHOUND_INDEXING__INCLUDE=*.py,*.js,*.ts

# File patterns to exclude (comma-separated)
# CHUNKHOUND_INDEXING__EXCLUDE=*.test.js,*.spec.ts

# Per-file timeout settings
# CHUNKHOUND_INDEXING__PER_FILE_TIMEOUT_SECONDS=30.0
# CHUNKHOUND_INDEXING__PER_FILE_TIMEOUT_MIN_SIZE_KB=100

# Modification time epsilon (seconds) - files changed within this window may be skipped
# CHUNKHOUND_INDEXING__MTIME_EPSILON_SECONDS=2.0

# ==============================================================================
# RESEARCH CONFIGURATION (for semantic code search)
# ==============================================================================

# Research algorithm: v3_parallel (default)
# CHUNKHOUND_RESEARCH_ALGORITHM=v3_parallel

# Query expansion settings
# CHUNKHOUND_RESEARCH_QUERY_EXPANSION_ENABLED=true
# CHUNKHOUND_RESEARCH_NUM_EXPANDED_QUERIES=3

# Search parameters
# CHUNKHOUND_RESEARCH_INITIAL_PAGE_SIZE=20
# CHUNKHOUND_RESEARCH_RELEVANCE_THRESHOLD=0.5
# CHUNKHOUND_RESEARCH_MAX_SYMBOLS=100

# Regex augmentation (combine semantic + regex search)
# CHUNKHOUND_RESEARCH_REGEX_AUGMENTATION_RATIO=0.3
# CHUNKHOUND_RESEARCH_REGEX_MIN_RESULTS=5
# CHUNKHOUND_RESEARCH_REGEX_SCAN_PAGE_SIZE=1000

# Multi-hop exploration
# CHUNKHOUND_RESEARCH_MULTI_HOP_TIME_LIMIT=30.0
# CHUNKHOUND_RESEARCH_MULTI_HOP_MAX_ITERATIONS=3
# CHUNKHOUND_RESEARCH_MULTI_HOP_MAX_CHUNKS=500

# ==============================================================================
# GENERAL CONFIGURATION
# ==============================================================================

# Enable debug logging
# CHUNKHOUND_DEBUG=false

# Config file path (alternative to --config CLI argument)
# CHUNKHOUND_CONFIG_FILE=/path/to/config.json

# ==============================================================================
# SPECIAL USE CASES
# ==============================================================================

# Custom OpenCode binary path (for codex-cli LLM provider)
# CHUNKHOUND_CODEX_BIN=opencode

# MCP mode flag (automatically set by MCP server - don't set manually)
# CHUNKHOUND_MCP_MODE=1

# YAML parser engine (tree or default) - for debugging
# CHUNKHOUND_YAML_ENGINE=default
